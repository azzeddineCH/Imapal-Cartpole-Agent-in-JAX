{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azzeddineCH/Imapal-Cartpole-Agent-with-Jax/blob/main/CartPole_with_impala_in_Jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import functools\n",
        "from typing import NamedTuple, Any, Tuple, List\n",
        "import dm_env\n",
        "import haiku as hk\n",
        "import jax.nn\n",
        "from jax import numpy as jnp\n",
        "import rlax\n",
        "import optax\n",
        "import distrax\n",
        "from typing import Callable\n",
        "from bsuite.environments import cartpole\n",
        "import threading\n",
        "import queue\n",
        "import numpy as np\n",
        "from rl.common import Transition\n",
        "from typing import NamedTuple, Any, Tuple\n",
        "import dm_env\n",
        "import haiku as hk\n",
        "import jax.nn\n",
        "from jax import numpy as jnp"
      ],
      "metadata": {
        "id": "sXpIW-l71J-4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class Transition(NamedTuple):\n",
        "    timestep: dm_env.TimeStep  # step_type [ First, Mid, Last ], reward, discount, observation\n",
        "    action: int\n",
        "    agent_out: Any\n",
        "\n",
        "\n",
        "class Network(hk.Module):\n",
        "\n",
        "    def __init__(self, num_actions: int):\n",
        "        super().__init__()\n",
        "        self._num_actions = num_actions\n",
        "\n",
        "    def __call__(self, timestep: dm_env.TimeStep) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "        torso = hk.Sequential([\n",
        "            hk.Flatten(),\n",
        "            hk.Linear(128), jax.nn.relu,\n",
        "            hk.Linear(64), jax.nn.relu\n",
        "        ])\n",
        "\n",
        "        hidden = torso(timestep.observation)  # batch_size, 64\n",
        "\n",
        "        policy_logit = hk.Linear(self._num_actions)(hidden)  # batch_size, num_actions\n",
        "        baseline = hk.Linear(1)(hidden)  # batch_size, 1\n",
        "        baseline = jnp.squeeze(baseline)  # batch_size\n",
        "\n",
        "        return policy_logit, baseline\n",
        "\n",
        "\n",
        "def preprocess_step(ts: dm_env.TimeStep) -> dm_env.TimeStep:\n",
        "    # reward: None -> 0, discount: None -> 1, scalar -> np.array(), StepType -> int.\n",
        "    if ts.reward is None:\n",
        "        ts = ts._replace(reward=0.)\n",
        "    if ts.discount is None:\n",
        "        ts = ts._replace(discount=1.)\n",
        "    return jax.tree_util.tree_map(jnp.asarray, ts)"
      ],
      "metadata": {
        "id": "B6HL9_3L1J-7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class Agent:\n",
        "\n",
        "    def __init__(self, net_apply_fn):\n",
        "        self._net = net_apply_fn\n",
        "        self._discount = 0.99\n",
        "\n",
        "    @functools.partial(jax.jit, static_argnums=0)\n",
        "    def step(\n",
        "            self,\n",
        "            params: hk.Params,\n",
        "            rng: jnp.ndarray,\n",
        "            timestep: dm_env.TimeStep\n",
        "    ):\n",
        "        timestep = jax.tree_util.tree_map(lambda t: jnp.expand_dims(t, 0), timestep)  # 1, ...observation_shape\n",
        "        logits, _ = self._net(params, timestep)  # 1, num_actions\n",
        "        logits = jnp.squeeze(logits)  # num_actions\n",
        "        action = hk.multinomial(rng, logits, num_samples=1)  # 1, num_samples\n",
        "        action = jnp.squeeze(action, axis=-1)  # num_samples,\n",
        "\n",
        "        return action, logits\n",
        "\n",
        "    def loss(self, params: hk.Params, trajs: Transition):\n",
        "        # 1- generate actions logits following the learner policy Pi\n",
        "        net_curried = hk.BatchApply(functools.partial(self._net, params))\n",
        "        learner_logits, baseline_with_boostrap = net_curried(\n",
        "            trajs.timestep)  # num_transitions, batch_size, num_actions, batch_size, 1\n",
        "\n",
        "        # 2- gather the learner Vt and Vt+1 for TD-error calculation\n",
        "        baseline = baseline_with_boostrap[:-1]  # V_t\n",
        "        learner_logits = learner_logits[:-1]  # Pi(a_t/s_t)\n",
        "        baseline_tp1 = baseline_with_boostrap[1:]  # V_t+1\n",
        "\n",
        "        # 3 - Remove bootstrapping timesteps t+1\n",
        "        _, behavior_actions, behavior_logits = jax.tree_util.tree_map(lambda t: t[:-1], trajs)\n",
        "\n",
        "        # 4 - Shift the behavior_timestep so that each element of behavior_actions matches the resulted timestep\n",
        "        behavior_timestep = jax.tree_util.tree_map(lambda t: t[1:], trajs.timestep)\n",
        "        discount = behavior_timestep.discount * self._discount\n",
        "\n",
        "        # 6 - Ignore the transition between Last to First timestep as it's a behavior of deepmind env api\n",
        "        mask = jnp.not_equal(behavior_timestep.step_type, int(dm_env.StepType.FIRST))\n",
        "        mask = mask.astype(jnp.float32)\n",
        "\n",
        "        # 7 - make the importance sampling ration Ci: Pi(a_t/s_t) / Mu(a_t/s_t)\n",
        "        rhos = distrax.importance_sampling_ratios(\n",
        "            target_dist=distrax.Categorical(learner_logits),\n",
        "            sampling_dist=distrax.Categorical(behavior_logits),\n",
        "            event=behavior_actions\n",
        "        )\n",
        "\n",
        "        # 8 - get the v_trace error and policy gradient advantage of each timestep\n",
        "        vtrace_returns = jax.vmap(rlax.vtrace_td_error_and_advantage, in_axes=1, out_axes=1)(\n",
        "            baseline,  # v_tm1\n",
        "            baseline_tp1,  # v_t\n",
        "            behavior_timestep.reward,  # r_t\n",
        "            discount,  # discount_t\n",
        "            rhos  # rho_tm1\n",
        "        )  # num_transition, batch_size\n",
        "\n",
        "        # 9 - Calculate the policy gradient loss\n",
        "        pg_advantage = jax.lax.stop_gradient(vtrace_returns.pg_advantage)\n",
        "        pg_loss = jnp.mean(\n",
        "            jax.vmap(rlax.policy_gradient_loss, in_axes=1, out_axes=0)(\n",
        "                learner_logits,  # logits_t\n",
        "                behavior_actions,  # a_t\n",
        "                pg_advantage,  # adv_t\n",
        "                mask,  # w_t\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # 10 - Calculate the baseline loss\n",
        "        bl_loss = 0.5 * jnp.mean(jnp.square(vtrace_returns.errors) * mask)\n",
        "\n",
        "        # 11 - Calculate the entropy loss\n",
        "        ent_loss = jnp.mean(jax.vmap(rlax.entropy_loss, in_axes=1, out_axes=0)(learner_logits, mask))\n",
        "\n",
        "        # 11 - Calculate the total weighted loss\n",
        "        total_loss = pg_loss + 0.5 * bl_loss + 0.01 * ent_loss\n",
        "        return total_loss"
      ],
      "metadata": {
        "id": "hgzpWce_1J-8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from rl.common import preprocess_step\n",
        "from typing import List\n",
        "\n",
        "\n",
        "def run_actor(\n",
        "        agent: Agent,\n",
        "        rng_key: jnp.ndarray,\n",
        "        get_params: Callable[[], hk.Params],\n",
        "        enqueue_traj: Callable[[Tuple[Transition, List[int]]], None],\n",
        "        horizon: int,\n",
        "        num_trajectories: int,\n",
        "):\n",
        "    env = cartpole.Cartpole()\n",
        "    state = env.reset()\n",
        "    traj = []\n",
        "    eps_return = 0\n",
        "    for i in range(num_trajectories):\n",
        "        params = get_params()  # get the latest params from the learner\n",
        "\n",
        "        eps_returns = []\n",
        "        for t in range(horizon + int(i == 0)):  # first rollout is one step longer, check L28\n",
        "            rng_key, step_key = jax.random.split(rng_key)\n",
        "            state = preprocess_step(state)\n",
        "            action, logits = agent.step(params, step_key, state)\n",
        "\n",
        "            transition = Transition(state, action, logits)\n",
        "            traj.append(transition)\n",
        "\n",
        "            state = env.step(action)\n",
        "\n",
        "            eps_return += state.reward if state.reward else 0\n",
        "            if state.step_type == dm_env.StepType.LAST:\n",
        "                eps_returns.append(eps_return)\n",
        "                eps_return = 0\n",
        "\n",
        "        trajectory_avg_return = jnp.mean(np.asarray(eps_returns))\n",
        "        stacked_traj = jax.tree_util.tree_map(lambda *ts: jnp.stack(ts), *traj)  # list of trees -> tree of list\n",
        "        enqueue_traj((stacked_traj, trajectory_avg_return))  # push the trajectory to the learner queue\n",
        "\n",
        "        traj = traj[-1:]  # resume the trajectory from the last transition"
      ],
      "metadata": {
        "id": "TAb12yaP1J-9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class Learner:\n",
        "\n",
        "    def __init__(self, agent: Agent, opt_update):\n",
        "        self._agent = agent\n",
        "        self._opt_update = opt_update\n",
        "\n",
        "    @functools.partial(jax.jit, static_argnums=0)\n",
        "    def update(\n",
        "            self,\n",
        "            params: hk.Params,\n",
        "            opt_state: optax.OptState,\n",
        "            trajs: Transition\n",
        "    ):\n",
        "        loss_value, gradient = jax.value_and_grad(self._agent.loss)(params, trajs)\n",
        "        updates, new_opt_state = self._opt_update(gradient, opt_state)\n",
        "        return optax.apply_updates(params, updates), new_opt_state, loss_value"
      ],
      "metadata": {
        "id": "Rejvf2Mx1J--"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def run(traj_per_actor, num_actors, horizon):\n",
        "    env = cartpole.Cartpole()\n",
        "    num_actions = env.action_spec().num_values\n",
        "\n",
        "    # create: Network, Impala Agent, Optimizer\n",
        "    net = hk.without_apply_rng(hk.transform(lambda ts: Network(num_actions)(ts)))\n",
        "    agent = Agent(net.apply)\n",
        "    opt = optax.rmsprop(learning_rate=5e-3, decay=0.99, eps=1e-7)\n",
        "\n",
        "    learner = Learner(agent, opt.update)\n",
        "\n",
        "    # Init the learner parameters\n",
        "    sample_timestep = env.reset()\n",
        "    sample_timestep = preprocess_step(sample_timestep)\n",
        "    ts_with_batch = jax.tree_util.tree_map(lambda t: jnp.expand_dims(t, 0), sample_timestep)\n",
        "    params = jax.jit(net.init)(jax.random.PRNGKey(69), ts_with_batch)\n",
        "\n",
        "    # Init Optimizer state\n",
        "    opt_state = opt.init(params)\n",
        "\n",
        "    # a utility callback to pull params from the learner\n",
        "    current_params = lambda: params\n",
        "\n",
        "    # Create the learner queue and dqueue utility method\n",
        "    # the dqueue method would wait until the element to remove\n",
        "    # is available in the queue to add it to the batch\n",
        "    batch_size = 2\n",
        "    q = queue.Queue(maxsize=batch_size)\n",
        "\n",
        "    def dequeue():\n",
        "        batch = []\n",
        "        batch_episode_returns = []\n",
        "        for _ in range(batch_size):\n",
        "            actor_trajectory, actor_trajectory_avg_return = q.get()  # { key: num_trasition, ... value_shape }\n",
        "            batch.append(actor_trajectory)\n",
        "            batch_episode_returns.append(actor_trajectory_avg_return)\n",
        "\n",
        "        batch = jax.tree_util.tree_map(lambda *ts: jnp.stack(ts, axis=1),\n",
        "                                       *batch)  # { key: num_trasition, batch_size, ... value_shape }\n",
        "\n",
        "        batch_avg_return = jnp.mean(np.asarray(batch_episode_returns))\n",
        "        return jax.device_put(batch), batch_avg_return\n",
        "\n",
        "    # Start the actors\n",
        "    for i in range(num_actors):\n",
        "        key = jax.random.PRNGKey(i)\n",
        "        args = (agent, key, current_params, q.put, horizon, traj_per_actor)\n",
        "        threading.Thread(target=run_actor, args=args).start()\n",
        "\n",
        "    # Start the learner\n",
        "    num_steps = num_actors * traj_per_actor // batch_size\n",
        "\n",
        "    for i in range(num_steps):\n",
        "        traj, avg_return = dequeue()\n",
        "        params, opt_state, loss_value = learner.update(params, opt_state, traj)\n",
        "        print(f\"step {i + 1}, loss: {loss_value}, avg return: {avg_return}\")\n"
      ],
      "metadata": {
        "id": "04YLlXIX1J-_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "run(traj_per_actor=500, num_actors=2, horizon=20)"
      ],
      "metadata": {
        "id": "cMMvnr1p1J-_"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}